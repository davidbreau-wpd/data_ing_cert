# **Data Lake partie 2 : ingestion avanc√©e, monitoring et s√©curit√©**

*Azure Data lake Storage Gen2, Azure Monitor, Azure Databricks, Microsoft Entra ID, Azure KeyVault*

## **Contexte Fictif**

En tant que **Data Ing√©nieur** au sein de l‚Äôentreprise **DataMoniSec**, vous √™tes charg√© de mettre en place une infrastructure de donn√©es robuste et s√©curis√©e sur **Microsoft Azure**. Votre mission s‚Äôarticule de la **s√©curit√©** et du **monitoring** du data lake.

Votre mission est de :

- **Configurer** un Data Lake pour centraliser les donn√©es de l'entreprise.
- **Ing√©rer** des donn√©es provenant de diff√©rentes sources.
- Mettre en place des mesures de **s√©curit√©** avanc√©es pour prot√©ger les donn√©es sensibles.
- Configurer **Azure Databricks** pour permettre √† l'√©quipe Data Science d'analyser les donn√©es.
- Impl√©menter un syst√®me de **monitoring** et d'**alertes** pour surveiller l'infrastructure.
- [Bonus] : **Spark, terraform, pricing, ingestion avanc√©e**

![4k6CeA7AWX4hyFHnwgzlY.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/44f1120f-8c68-4152-889c-4e32ed83862a/264d962d-477d-4b3a-896e-36aab9b033d1/8f7c6dd3-08d0-4f9f-b23b-859b78af8f5f.png)

---

## **Partie 1 : Veille sur les Syst√®mes de S√©curit√©**

### **Objectif**

Acqu√©rir une **vision panoramique** des m√©thodes de s√©curit√© disponibles sur Azure pour prot√©ger les donn√©es dans un Data Lake. Je ne vous demande pas d‚Äô√™tre un expert sur chacun de ces √©l√©ments mais d‚Äôavoir une intuition de comment √ßa marche et de pouvoir d√©crire en une phrase ce que fait chaque service. 

Vous pouvez √©galement vous lancez dans le projet et vous formez sur ces concepts au fur et √† mesure.

### **Veille**

***Important pour le projet :*** 

- **Storage Access Keys**
- **Shared Access Signatures (SAS) (Delegation Key)**
- **Microsoft Entra ID** (anciennement Azure Active Directory) (service principal)
- **Azure Key Vault**
- **IAM et Role-Based Access Control (RBAC)**

**Ressources** : Utilisez des outils tels que Google, la documentation officielle d'Azure, des mod√®les de langage (LLMs), et autres sources fiables. Je vous recommande d‚Äôutiliser le nouvel outil **Learning about** de Google (un LLM sp√©cial pour apprendre sur un sujet, il n√©cessite un VPN car il est disponible qu‚Äôaux √©tats-unis)

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/44f1120f-8c68-4152-889c-4e32ed83862a/0a67f82c-c19a-4c65-9277-3c3c192fb035/image.png)

### **Livrable**

- Un rapport qui synth√©tise vos recherches en d√©crivant chaque m√©thode de s√©curit√©, ses avantages, ses limitations, et des cas d'utilisation.
- [Bonus] Sur la base du volotariat, un apprenant pourra expliquer un ou plusieurs des composants aux autres (5 minutes maximum)

---

## **Partie 2 : Cr√©ation et ingestion s√©curis√©e sur le data lake**

### **Objectif**

L‚Äôobjectif de cette partie est de cr√©er des scripts Python pour stocker des donn√©es dans un **`Azure Data Lake Storage Gen 2`** de mani√®re s√©curis√©e. Vous allez vous concentrer sur la gestion des identit√©s et des secrets en utilisant des outils comme **`Azure Key Vault`**, tout en pratiquant les concepts de s√©curit√© avec Azure.

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/44f1120f-8c68-4152-889c-4e32ed83862a/7bff4506-d080-4b90-acf1-460695b0575c/image.png)

### Concr√®tement :

1. **T√©l√©charger et stocker des fichiers CSV ou Parquet** sur le Data Lake.
2. **Cr√©er une configuration s√©curis√©e** pour g√©n√©rer et utiliser des **SAS Tokens** afin de upload des fichiers.
3. **Explorer une approche avec deux Service Principals (SP)** pour renforcer la s√©curit√© et pratiquer l‚Äôutilisation de Key Vault.

<aside>
üí°

**Pourquoi deux SP ?**

C‚Äôest un peu comme des poup√©es russes ü™Ü : chaque couche ajoute une barri√®re suppl√©mentaire pour prot√©ger vos donn√©es. Cette configuration peut sembler un peu complexe au premier abord (et je comprends pourquoi üòÖ), mais elle a un objectif clair‚ÄØ: **r√©duire l‚Äôexposition des secrets sensibles** tout en vous faisant pratiquer des concepts importants comme Key Vault, les r√¥les Azure, et la gestion des SAS Tokens.

---

### **Logique de cette configuration**

1. **SP Secondaire (Acc√®s √† Key Vault)** :
    - Ce SP a un r√¥le ultra-limit√©‚ÄØ: acc√©der uniquement au secret stock√© dans Key Vault. Il ne peut pas manipuler directement les donn√©es du Data Lake.
    - Pourquoi‚ÄØ? Si un secret doit √™tre expos√© dans l‚Äôapplication, autant que ce soit celui d‚Äôun SP qui n‚Äôa **aucun acc√®s direct** au Data Lake. Cela limite l‚Äôimpact d‚Äôune compromission.
    - De plus, gr√¢ce √† **Key Vault**, vous b√©n√©ficiez de fonctionnalit√©s comme :
        - **Journalisation des acc√®s** : Qui a acc√©d√© √† quel secret, quand et depuis o√π.
        - **Rotation des cl√©s** : En cas de fuite, vous pouvez r√©g√©n√©rer les cl√©s sans toucher au reste de l‚Äôinfrastructure.
2. **SP Principal (Acc√®s au Data Lake)** :
    - Ce SP est plus puissant‚ÄØ: il g√©n√®re les **SAS Tokens** n√©cessaires pour manipuler les donn√©es. Cependant, son mot de passe (ou secret) est **stock√© dans Key Vault**, donc inaccessible directement depuis l‚Äôapplication.
    - Pourquoi‚ÄØ? Cette s√©paration permet de ne jamais exposer directement les informations sensibles d‚Äôun SP puissant. Vous isolez les responsabilit√©s et r√©duisez les surfaces d‚Äôattaque.

---

### **Avantages de cette approche**

1. **Limitation des privil√®ges expos√©s** :
    - L‚Äôapplication n‚Äôa acc√®s qu‚Äô√† un SP (le Secondaire) avec une permission limit√©e : `get` sur les secrets. C‚Äôest comme donner une cl√© qui ouvre seulement une bo√Æte sp√©cifique.
    - Le SP Principal est prot√©g√© par Key Vault‚ÄØ: son mot de passe n‚Äôest jamais directement dans le code.
2. **Audit et contr√¥le** :
    - Key Vault permet d‚Äôauditer toutes les actions‚ÄØ: chaque tentative de lecture ou d‚Äôacc√®s est journalis√©e.
    - Si une fuite est d√©tect√©e, vous savez **exactement qui a acc√©d√© aux secrets** et pouvez r√©agir rapidement.
3. **Rotation simplifi√©e des cl√©s** :
    - En cas de fuite, vous pouvez reg√©n√©rer toutes les cl√©s dans Key Vault sans toucher √† l‚Äôapplication.
    - Seule la cl√© d‚Äôacc√®s au SP Secondaire devra √™tre mise √† jour dans l‚Äôapplication.

---

### **Pourquoi cette approche est coh√©rente**

Certes, on d√©place le probl√®me (il faut toujours un secret dans l‚Äôapplication, celui du SP Secondaire). Mais‚ÄØ:

1. Ce secret est **moins critique**‚ÄØ: il n‚Äôoffre aucun acc√®s direct au Data Lake.
2. Le point d‚Äôentr√©e est **Key Vault**, qui est con√ßu pour s√©curiser et journaliser les acc√®s.
3. La s√©paration des SP garantit que, m√™me si le SP Secondaire est compromis, l‚Äôacc√®s au Data Lake reste prot√©g√©.

---

### **Conclusion**

Cette configuration peut para√Ætre un peu compliqu√©e, mais elle est une **bonne pratique de s√©curit√©** :

- Vous minimisez les impacts d‚Äôune compromission.
- Vous utilisez les outils Azure (Key Vault, journaux, rotation) pour g√©rer les secrets efficacement.

Et soyons honn√™tes : le vrai but ici, c‚Äôest aussi de vous faire manipuler ces concepts et outils ! üòâ

</aside>

---

### **Activit√©s**

### **√âtape 1 : Configuration Initiale**

1. **Cr√©ez un Data Lake Storage Gen 2** dans votre groupe de ressources (par exemple, `RG_JVANGANSBERG`).
2. Configurez le **conteneur de stockage** pour accueillir vos fichiers. (exemple : `data`)
3. Cr√©er un Key Vault `keyvault`[`initial_pr√©nom` `nomdefamille`]
4. Effectuer la configuration s√©curis√©e (voir **Guide de Configuration S√©curis√©e** plus bas**)**

### **√âtape 2 : Travail avec les donn√©es**

Vous allez manipuler deux types de donn√©es :

- **Donn√©es CSV** : [Inside Airbnb - Get the Data](https://insideairbnb.com/get-the-data/).
- **Donn√©es Parquet** : [Hugging Face Datasets](https://huggingface.co/datasets/Marqo/amazon-products-eval/tree/main/data).

---

### **Niveaux de Difficult√©**

**A. Donn√©es CSV depuis Inside Airbnb**

1. **Niveau 1** : Faire un script qui va stocker un csv sur le data lake. Vous pouvez aller sur le lien plus haut afin de t√©l√©charger un csv relatif √† la ville de **Barcelone** (vous pouvez prendre celui que vous voulez mais disons [reviews.csv](https://data.insideairbnb.com/spain/catalonia/barcelona/2024-09-06/visualisations/reviews.csv) si vous ne savez pas lequel prendre). Pour sauvegarder le csv sur le data lake vous allez faire un script d‚Äôingestion qui va g√©n√©rer un SAS mais vous ne devez pas exposer la SAK. Vous cr√©erez au pr√©alable deux services principales pour g√©n√©rer ce SAS de fa√ßon s√©curis√©e.
    
    *Note : Pour ce point vous avez le choix entre d‚Äôabord t√©l√©charger le fichier en local ou pas.*
    
2. **Niveau 2** : Faire la m√™me chose mais en faisant un script qui va t√©l√©charger toutes les donn√©es relatif √† l‚Äô`espagne` .
3. **[Facultatif]** : Faire la m√™me chose avec `Azure Data Factory`. Comme c‚Äôest un service Azure vous n‚Äôaurez pas besoin de la configuration du niveau 1, faites le uniquement si vous voulez revoir `Azure Data Factory` mais le focus de cette semaine c‚Äôest la s√©curit√© et le monitoring.

**B. Donn√©es Parquet depuis Hugging Face**

1. **Niveau 1** : Pareil qu‚Äôau point A avec un fichier parquet
2. **Niveau 2** : Tous (ou un batch de plusieurs 5 √† 20) les fichiers parquet
3. **Niveau 3** : Avec Data Factory

La configuration du B sera quasiment identique, c‚Äôest juste pour manipuler un fichier parquet plus tard dans le brief.

---

### **Guide de Configuration S√©curis√©e**

### **1. Cr√©ez deux Service Principals (SP)**

- **SP Secondaire** :
    - Nom: `sp-keyvault-[Initial pr√©nom+nomdefamille]`
    - Strat√©gie d‚Äôacc√®s (*acces policies*): Acc√®s √† `Azure Key Vault` (uniquement `get`).
    - Permet de r√©cup√©rer les secrets (mot de passe du SP principal).
- **SP Principal** :
    - Nom: `sp-datalake-[Initial pr√©nom+nomdefamille]`
    - R√¥le‚ÄØ: `Storage Blob Data Contributor`

### **2. Configurez Key Vault**

- Stockez dans Key Vault le **secret du SP Principal**.
- Attribuez une **Access Policy** ou un r√¥le **Key Vault Secrets User** au SP Secondaire pour qu‚Äôil puisse acc√©der au secret.

### **3. G√©n√©ration des SAS Tokens**

- Utilisez le SP Principal pour g√©n√©rer une User Delegation Key.
- Avec cette cl√©, cr√©ez des **SAS Tokens** qui permettent d‚Äô√©crire ou de lire des fichiers dans le Data Lake.

Dans votre application (le script de upload) vous ne devrez stocker les informations suivantes :

```python
SP_ID_SECONDARY
SP_SECONDARY_PASSWORD
SP_ID_PRINCIPAL
TENANT_ID
KEYVAULT_URL
SECRET_NAME
STORAGE_ACCOUNT_NAME
```

Rendez votre code modulaire, le code qui vous permettra de g√©n√©rer un SAS sera commun aux fichiers parquet et aux fichiers csv. √áa repr√©sente une opportunit√© pour modulariser votre code.

## Journalisation

L'un des avantages d'utiliser **Azure Key Vault** comme point d'entr√©e est la possibilit√© de **journaliser les interactions** avec celui-ci. Vous pouvez activer cette fonctionnalit√© via les **param√®tres de diagnostic**. Il existe diff√©rentes mani√®res de sauvegarder ces journaux, et l'une des plus simples est de les **stocker en tant qu'archives dans un compte de stockage**.

> Remarque : Bien que vous puissiez stocker les journaux sur le data lake, il est d√©conseill√© de les conserver sur le m√™me data lake que vous cherchez √† s√©curiser. Cela pourrait pr√©senter un risque de s√©curit√© en exposant les d√©tails des acc√®s et des op√©rations sur le data lake lui-m√™me.
> 

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/44f1120f-8c68-4152-889c-4e32ed83862a/93b398f9-43b1-4703-85ab-d50abbf6516b/image.png)

---

### **Livrables**

1. **Repertoire du projet sur github**
2. **Documentation** :
    - Expliquez les r√¥les des SP et la configuration.
    - Justifiez les permissions attribu√©es.
3. **Journal Key Vault** : Fournissez les logs qui montrent les actions sur les secrets.

---

## **Partie 3 : Configuration d'Azure Databricks**

### **Objectif**

Configurer Azure Databricks pour permettre √† l'√©quipe Data Science d'analyser les donn√©es, tout en mettant en ≈ìuvre les mesures de s√©curit√© appropri√©es. 

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/44f1120f-8c68-4152-889c-4e32ed83862a/c8550193-3cc5-4d5a-b099-a40f93a5bbbc/image.png)

Cette partie s‚Äôappuye sur ces tutos microsofts que vous devrez suivre en d√©tail : 

- https://learn.microsoft.com/en-us/azure/databricks/connect/storage/tutorial-azure-storage
- https://learn.microsoft.com/en-us/azure/databricks/dbfs/mounts?source=recommendations

Ces tutoriels font appels √† plusieurs outils Azure que vous aurez explorer plus haut durant votre veille.

### **Activit√©s**

- Cr√©er la ressource : `databricks-`[`initial_prenom` `nom_de_famille`] S√©lectionner bien la version ‚ÄúStandard‚Äù et surtout pas ‚ÄúPremium‚Äù.
- **Configurer le cluster Azure Databricks** avec la configuration suivante :
    - **Version** : **15.4 LTS**
    - **Type de Cluster** : **Standard-DS3_v2** (prix max : **0.75 DBU/h**)
    - **Arr√™t Automatique** : Apr√®s **1 heure** d'inactivit√©
    - **Mode d'Acc√®s** : **Single User**
- **Int√©grer les Composants de S√©curit√©** :
    - **Azure Key Vault** : Pour stocker et g√©rer les secrets et les cl√©s de chiffrement.
    - **Microsoft Entra ID** : Pour l'authentification et l'autorisation des utilisateurs.
    - **R√¥les IAM** : Pour d√©finir les permissions d'acc√®s aux ressources.
- **Monter le Data Lake dans Databricks** :
    - Utiliser les bonnes pratiques pour connecter `Azure Databricks` √† `Azure Data Lake Storage Gen2` en utilisant les m√©thodes s√©curis√©es (par exemple, en utilisant `Azure Key Vault` pour stocker les credentials).
- **Charger un Fichier CSV ou Parquet** :
    - Utiliser le code Spark pour charger le fichier depuis le Data Lake et afficher les 20 premi√®res lignes :
        
        ```python
        df = spark.read.format("csv") \
            .option("header", "true") \
            .option("inferSchema", "true") \
            .load(file_path)
        
        df.show()
        ```
        
    - Le code spark obligatoire pour ce projet s‚Äôarr√™te l√†, donc pas besoin d‚Äôavoir des connaissances au pr√©alable. Nous d√©dierons un temps pour apprendre Spark plus tard dans la formation. Si vous le souhaitez vous pourrez, aller plus loin d√®s maintenant (voir bonus)

### **Livrables**

- **Documentation** : D√©crivez les √©tapes de configuration, les challenges rencontr√©s, et comment vous les avez r√©solus.
- **Notebook Databricks** : Contenant le code utilis√© pour monter le Data Lake et charger les donn√©es.

---

## **Partie 4 : Monitoring et Alertes**

### **Objectif**

Mettre en place un syst√®me de monitoring et d'alertes pour surveiller le Data Lake et √™tre en mesure de r√©agir rapidement en cas d'incidents.

L‚Äôobjectif de cette partie est de d√©couvrir `Logs Analytics`, `Activity logs`, `Metrics`, `insights` et `Alerts`. Ce sont des services que vous pouvez atteindre de diff√©rentes fa√ßons. Par exemple, dans votre `storage account`, il y a un section `monitoring` avec ces services pr√©-configur√©s pour surveiller votre instance de `storage account`. Vous pouvez √©galement utiliser ces services de mani√©re ind√©pendante, en faisant par exemple une recherche `Metrics` dans la barre de recherche. Ou enfin vous pouvez les utiliser via le service unifi√© `Monitor`.

Vous pouvez effectuer une veille avant d‚Äôutiliser ces outils ou alors au fur-et-√†-mesure que vous les utilisez.

- **Activity Logs** :
    - **Objectif** : Analysez les actions effectu√©es sur le Data Lake durant la partie pr√©c√©dente
    - **Appliquer des Filtres** : Utilisez les filtres pour vous concentrer sur les √©v√©nements pertinents : les √©v√©nements qui ont √©t√© cr√©√© par votre compte, ou sur vos ressources (adls, databricks)
    - **Exporter les Logs** : Configurez l'exportation des Activity Logs vers un **Log Analytics Workspace**.
- **Metrics**:
    - **Identifier les M√©triques Cl√©s** : Par exemple, le d√©bit entrant/sortant, la latence, le nombre de requ√™tes r√©ussies/√©chou√©es.
    - **Cr√©er un Dashboard** : Utilisez `Azure Monitor` pour cr√©er un dashboard personnalis√© pour le Data Lake.
- **Insights** :
    - **Utiliser Azure Storage Insights** : Pour obtenir une vue d√©taill√©e des performances et de l'utilisation du Data Lake.
    - Quel est la diff√©rence entre `Metrics` et `Insights` ?
    - **Comprendre les Latences** : C‚Äôest quoi la diff√©rence entre E2E latency et server latency ?
        - **E2E Latency** : Temps total pour qu'une requ√™te soit trait√©e du d√©but √† la fin.
        - **Server Latency** : Temps que le serveur met √† traiter la requ√™te, excluant les d√©lais de r√©seau.
        
        Sch√©ma pour vous aider :
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/44f1120f-8c68-4152-889c-4e32ed83862a/f38eda97-32fa-4fc9-8e56-0d0d92841a16/image.png)
        
        
- **Alerts** :
    - **Configurer des Alertes sur les M√©triques** :
        - **Exemple** : D√©finir une alerte lorsque le trafic entrant (Ingress) d√©passe **1 Go**. Testez son d√©clenchement en simulant l'√©v√©nement.
    - **Configurer des Alertes sur les Activity Logs** :
        - **Exemple** : Avertir l'administrateur lorsque les **Storage Access Keys** sont r√©g√©n√©r√©es.
    - **D√©finir des R√®gles par S√©v√©rit√©** :
        - **S√©v√©rit√© 1-3** : D√©finir que le d√©passement du seuil ingress est de s√©v√©rit√© plus faible et donc envoie uniquement un **email** √† l'administrateur.
        - **S√©v√©rit√© 4-5** : D√©finir que la r√©g√©n√©ration des cl√©s et plus grave et donc il va envoyer un **email** et d'un **SMS**.
    - Dans cette partie, on consid√®re que vous √™tes l‚Äôadministrateur, configurez donc votre email et votre num√©ro de t√©l√©phone. Pour effectuer ces tests.

### **Livrables**

- **Captures d'√âcran** : Des dashboards, des alertes configur√©es, et des notifications re√ßues.
- **Rapport** : D√©crivant le syst√®me de monitoring mis en place, les raisons des choix effectu√©s, et comment le syst√®me r√©pond aux besoins de l'entreprise.

---

## **Partie 5 : Bonus**

### **Option 1 : Ingestion Avanc√©e (niveau 2)**

- **Objectif** : Automatiser l'ingestion des donn√©es avec des scripts avanc√©s ou des pipelines.
- **Activit√©s** :
    - **Pour Inside Airbnb** :
        - √âcrire un **script Python** qui parse la page HTML pour extraire les URLs des fichiers li√©s √† l'Espagne (par exemple, en utilisant des expressions r√©guli√®res ou des biblioth√®ques comme BeautifulSoup).
        - Utiliser `**Azure Data Factory**` pour cr√©er un pipeline avec un **ForEach** qui t√©l√©charge chaque fichier et l'ing√®re dans le Data Lake.
    - **Pour Hugging Face** :
        - Automatiser le t√©l√©chargement de plusieurs fichiers Parquet en utilisant des scripts ou Azure Data Factory.
        - G√©rer les exceptions et les cas o√π certains fichiers ne sont pas disponibles.

### **Option 2 : Exploration des Donn√©es avec Spark**

- **Objectif** : Approfondir l'analyse des donn√©es en utilisant **PySpark**.
- **Activit√©s** :
    - Charger les donn√©es dans un DataFrame Spark.
    - Effectuer des op√©rations d'analyse : filtrage, agr√©gation, jointures.
    - Visualiser les r√©sultats directement dans le notebook.
    - **Ressource** : Suivre un tutoriel vid√©o recommand√© (lien fourni par le formateur).

### **Option 3 : R√©vocation des Acc√®s en Cas d'Incident**

- **Objectif** : Automatiser la r√©ponse aux incidents critiques.
- **Activit√©s** :
    - Utiliser `Azure Logic Apps` ou`Azure Functions` pour cr√©er un workflow qui r√©voque automatiquement les acc√®s au Data Lake en cas d'alerte critique.
    - Int√©grer ce workflow avec le syst√®me d'alertes configur√© pr√©c√©demment.

### **Option 4 : Inspection du Pricing**

- **Objectif** : Comprendre et optimiser les co√ªts associ√©s √† vos configurations Azure.
- **Activit√©s** :
    - **Analyser le Co√ªt** de la configuration Azure Databricks mise en place :
        - Utiliser la **Calculatrice de Prix Azure** pour estimer le co√ªt mensuel.
        - Identifier les principaux facteurs de co√ªt.
    - **Optimiser les Co√ªts** :
        - Explorer des options telles que la mise √† l'√©chelle automatique, l'utilisation d'instances spot, ou la r√©duction de la taille du cluster.
        - Proposer des recommandations pour √©quilibrer performance et co√ªt.

### **Option 5 : Utilisation de terraform**

- **Objectif** : Utiliser terraform pour g√©rer vos ressources Azure.