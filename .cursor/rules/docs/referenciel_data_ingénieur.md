CERTIFICATION DATA INGÉNIEUR
- - - Liste des compétences à valider 


# ██ Bloc B1 : Pilotage


## ═══ E1 : GRILLE D'ENTRETIENS


#### 'C1. Analyser l’expression d’un besoin de projet data dans une étude de faisabilité en explorant, à l’aune des enjeux stratégiques de l’organisation, le besoin métier avec les parties prenantes pour valider les orientations et sélectionner les hypothèses techniques du projet avec le ou les commanditaire(s). '

* Critères d'évaluation : " Les grilles d’entretiens questionnent les activités métiers impliquées dans le projet data  Les grilles d’entretiens questionnent la caractérisation des données impliquées, des métadonnées, des accès, des stockages et des traitements appliqués.  La note de synthèse rend compte du questionnement et de l’analyse du besoin, du périmètre fonctionnel du projet, des moyens disponibles, de la faisabilité et de la gouvernance de la donnée liée au projet.  La note de synthèse est organisée, par exemple selon le plan suivant : 1. intro : rappel du contexte, enjeux du projet, reformulation de l’expression de besoin initiale, annonce du plan 2. objectifs et périmètre fonctionnel du projet 3. étude d’opportunités : a. synthèse des informations issues des entretiens métier au regard des objectifs du projet : opportunités ? Contraintes ? b. benchmark des solutions et projets existants dans le périmètre fonctionnel visé  4. étude de la faisabilité : rapport objectifs, qualité, coûts, délais, moyens mobilisables. 5. conclusions : analyse RICE*  Les objectifs du projet sont rédigés selon la méthode SMART*  Le cadrage projet reprend et complète la note de synthèse : hypothèses et préconisations macro de solutions techniques, évaluation des aménagements à implémenter pour l’accessibilité du projet dans sa réalisation et dans l’utilisation future de ses produits, les actions pour la mise en conformité avec le RGPD.  L’effort d’accessibilité tout au long du projet est anticipé dans l’avant-projet, concernant : - la réalisation du projet : adaptation des postes de travail de l’équipe technique, - les utilisateurs finaux : adaptation des interfaces homme-machine des outils et des supports techniques (documentations, communications...).  L'avant-projet liste les actions techniques et non-techniques à mener tout au long du projet pour assurer la conformité du projet avec le RGPD. "



## ═══ E2 : Analyser un besoin Data


#### 'C2. Cartographier les données disponibles en référençant les usages, les sources, les métadonnées et les données, afin de valider les hypothèses techniques du projet data  '

* Critères d'évaluation : "La topographie des données est complète, respectant la structuration suivante en quatre parties : - La sémantique : les métadonnées des données et les objets métier propres à l’organisation dans un glossaire métier. - Les modèles de données : la façon dont les données sont modélisées et stockées dans les différents systèmes (structurées, semi structurées ou non structurées). - les traitements et les flux de données : les informations importantes sur les méthodes de transformation, de manipulation et de traitement des données à travers les différents SI de l’organisation. - la mise à disposition, les accès et les conditions d’utilisation des données.```


### ' C3. Concevoir un cadre technique d’exploitation des données en analysant les contraintes techniques, de moyens et la cartographie des données afin de définir une réponse technique adaptée aux ressources mobilisables dans le respect du RGPD et d’une démarche éco-responsable. '

* Critère d'évaluation : " - L’analyse fonctionnelle : Que fait le système décrit ? Quelles sont les contraintes métiers qui auront un impact sur l’architecture ? - Les besoins non-fonctionnels (outils et contraintes techniques) - La représentation fonctionnelle - La représentation applicative - La représentation d’infrastructure - La représentation opérationnelle - Les décisions d’architecture : documentation des choix techniques effectués - Les processus de mise en conformité RGPD (registre des données personnelles, traitements des tris et suppression, etc.) - La stratégie d’éco-responsabilité selon le référentiel général d’éco-conception des services numériques du gouvernemen - Les risques et les coûts  Les choix techniques d’architecture prennent en compte et respectent les objectifs du projet data les moyens mobilisables.  L’accessibilité des livrables du projet est anticipée et des solutions d'adaptation de poste de travail sont proposées quand cela est nécessaire.  La représentation applicative contient une matrice des flux.  La matrice des flux permet la qualification des flux de données concernées par le projet.  Les choix dans l’étude technique d’architecture s'appuient sur les ressources techniques et matériels du SI directement mobilisables pour le projet.  Le choix des éventuels prestataires techniques intègre le paramètre d’adoption d’une démarche d’éco-responsabilité chez le ou les prestataires,  Le choix des outils et services techniques concourent au respect de la stratégie d’éco-conception du projet  L‘étude techniques d’architectures définit et illustre les processus nécessaires à implémenter lors de la réalisation du projet pour la conformité avec le RGPD  Les processus de conformité définis par l’étude technique d’architecture sont conformes au RGPD "


#### 'C4. Réaliser une veille technique et réglementaire en sélectionnant des sources et en collectant et traitant les informations collectées afin de formuler des recommandations projet toujours en phase avec l’état de l’art. '

* Critères d'évalutation : "La thématique de veille choisie porte sur un outil et/ou une réglementation mobilisée dans la mise en situation.  Les temps de veille sont planifiés régulièrement (à minima une récurrence d’une heure hebdomadaire).  Le choix des outils d’agrégation est cohérent avec les sources d’informations visées et le budget disponible (flux RSS, flux réseaux sociaux, agrégation newsletter, etc.).  Les synthèses sont communiquées aux parties prenantes dans un format qui respecte les recommandations d’accessibilité (par exemple celles de l’association Valentin Haüy ou d’Atalan - AcceDe).  Les informations partagées dans la synthèse répondent à la thématique de veille choisie.  Les sources et flux identifiés répondent aux critères de fiabilité :  L’auteur de la page est identifié.  Des informations sur l’auteur sont disponibles et confirment ses compétences, sa notoriété et l'absence d'intérêts personnels.  L'analyse du contenu est valable (date de publication récente, sources de l'information indiquées, niveau de  langue correct).  La source (site) ou le document est structuré.  Les sources (sites) ou documents respectant les normes d'accessibilité sont privilégiés.  L'information peut être confirmée par d'autres sites de confiance. "



## ═══ E3 : Lancement de projet Data


### 'C5. Planifier la réalisation d’un projet data en attribuant les moyens nécessaires et en définissant les étapes de réalisation et les méthodes de suivi du projet afin d’organiser sa mise en œuvre. '

* Critères d'évaluation : "La préconisation de composition d’équipe couvre en compétences les besoins nécessaires à la réalisation du projet.  L'équipe projet est composée selon les compétences nécessaires à la réalisation du projet et prend en compte le budget affecté.  Les moyens financiers alloués correspondent au budget prévisionnel du projet.  La feuille de route est découpée en grandes étapes de réalisation du projet.  Les grandes étapes de la feuille de route respectent les ensembles fonctionnels du projet.  Le calendrier rend compte : des tâches et livrables attendus,  des dates d’échéance associées,  de l’attribution des ressources,  de la pondération des efforts nécessaires à la réalisation des tâches. des rituels d’animation du travail collaboratif de l’équipe projet sont également inclus.   Le suivi du calendrier permet d’atteindre les objectifs du projet en respectant les contraintes.  L’attribution des ressources est cohérente avec la répartition des acquis et non-acquis des compétences au sein des membres de l’équipe.  La pondération est réalisée selon une méthode choisie et partagée avec l’équipe (poker planning, méthode de l’unité équivalente, etc.).  Le paramétrage des outils de suivi est cohérent avec les délais et les attributions de missions du planning.  Les outils de suivi intègrent les indicateurs de suivi.  Les éléments de planification sont communiqués à l’équipe dans un format qui respecte les recommandations d’accessibilité (par exemple celles de l’association Valentin Haüy ou d’Atalan - AcceDe).  L'enchaînement des tâches permet la réalisation de chacune d'entre elles. "


### 'C6. Superviser la réalisation d’un projet data en organisant les méthodes, les outils de travail et la communication entre les parties prenantes, afin d’accompagner les membres de l’équipes dans la réussite du projet'

* Critères d'évaluation : "L’animation des échanges est adaptée à l’information à transmettre : les supports et les modalités d’animation sont adaptés aux besoins de communication.  Toutes les personnes concernées sont impliquées dans les échanges. Le contenu et le discours sont adaptés au public et au contexte des échanges.  Les outils de suivi sont configurés et accessibles à toutes les parties prenantes.  Les outils de suivi respectant les normes d'accessibilité (par exemple : RGAA) sont privilégiés.  Les rituels sont documentés : règles de participation et d’organisation aux rituels, planification, etc.  Les indicateurs de suivi sont mis à jour tout au long du projet, lors des rituels de suivi de l’avancement du projet.  Les dépenses sont imputées au budget prévisionnel tout au long du projet.  Les commandes communiqués aux prestataires externes couvrent les objectifs, la méthodologies de travail, les outils à utiliser, les livrables et les critères d’acceptance."


### 'C7. Communiquer tout au long de la réalisation du projet data sur les orientations, les réalisations et leurs impacts en élaborant la stratégie et les supports de communication afin d’informer toutes les parties prenantes des évolutions ou des opportunités internes comme externes, portés par le projet'

* Critères d'évaluation : 'Toutes les étapes de communication du projet sont planifiées : au lancement, à chaque jalon de la feuille de route, pour les démonstrations, à la livraison du projet.  Les supports de communication sont accessibles à toutes les parties prenantes.  Les supports de communications respectent les préconisations de mise en page sur des critères d’accessibilités (par exemples celles de l’association Valentin Haüy ou de Atalan - AcceDe ). 6 7  Toutes les personnes concernées sont impliquées dans les échanges.  Le contenu et le discours sont adaptés au public et au contexte des échanges.  Les communications présentent les orientations choisies et arbitrages menés pour la réalisation du projet.  Les tâches de production de la documentation utilisateurs sont planifiées et réparties entre les membres de l’équipe.  Les temps d’accompagnement des utilisateurs finaux sont planifiés.  Les missions des utilisateurs invités aux temps d’accompagnement sont cohérentes avec la prise en main du livrable visée.  Le recueil des retours des parties prenantes et leur traitement suit un processus qui est intégré à la stratégie de communication du projet.'




#  ██ B2 : API Rest


## ═══ E4 : API Rest


### 'C8. Automatiser l’extraction de données depuis un service web, une page web (scraping), un fichier de données, une base de données et un système big data en programmant le script adapté afin de pérenniser la collecte des données nécessaires au projet.'

* Critères d'évaluation : "La présentation du projet et de son contexte est complète : acteurs, objectifs fonctionnels et techniques, environnements et contraintes techniques, budget, organisation du travail et planification.  Les spécifications techniques précisent : les technologies et outils, les services externes, les exigences de programmation (langages), l’accessibilité (disponibilité, accès).  Le périmètre des spécifications techniques est complet : il couvre l’ensemble des moyens techniques à mettre en œuvre pour l’extraction et l'agrégation des données en un jeu de données brutes final.  Le script d’extraction des données est fonctionnel : toutes les données visées sont effectivement récupérées à l’issue de l’exécution du script.  Le script comprend un point de lancement, l’initialisation des dépendances et des connexions externes, les règles logiques de traitement, la gestion des erreurs et des exceptions, la fin du traitement et la sauvegarde des résultats.  Le script d’extraction des données est versionné* et accessible depuis un dépôt Git*.  L’extraction des données est faite depuis un mix entre au moins les sources suivantes : un service web (API REST), un fichier de données, un scraping, une base de données et un système big data."


### 'C9. Développer des requêtes de type SQL* d’extraction des données depuis un système de gestion de base de données et un système big data en appliquant le langage de requête propre au système afin de préparer la collecte des données nécessaires au projet.'

* Critères d'évaluation : "Les requêtes de type SQL pour la collecte de données sont fonctionnelles : les données visées sont effectivement extraites suites à l'exécution des requêtes.  La documentation des requêtes met en lumières choix de sélections filtrages, conditions, jointures, etc., en fonction des objectifs de collecte.  La documentation explicite les optimisations appliquées aux requêtes."


## "C10. Développer des règles d'agrégation de données issues de différentes sources en programmant, sous forme de script, la suppression des entrées corrompues et en programmant l’homogénéisation des formats des données afin de préparer le stockage du jeu de données final."

* Critères d'évaluation : 'Le script d’agrégation des données est fonctionnel : les données sont effectivement agrégées, nettoyées et normalisées en un seul jeu de données à l’issue de l’exécution du script. Le script d’agrégation des données est versionné et accessible depuis un dépôt Git. La documentation du script d’agrégation est complète : dépendances, commandes, les enchaînements logiques de l’algorithme, les choix de nettoyage et d’homogénéisation des formats données.'


### 'C11. Créer une base de données dans le respect du RGPD en élaborant les modèles conceptuels et physique des donnees à partir des donnees preparees et en programmant leur import afin de stocker le jeu de donnees du projet.'

* Critères d'évaluation : "Les modélisations des données respectent la méthode et le formalisme MERISE.  Le modèle physique des données est fonctionnel : il est intégré avec succès lors de la création de la base de données, sans erreur.   La base de données est choisie au regard de la modélisation des données et des contraintes du projet.   La reproduction des procédures d’installation décrites (base de données et API) a pour résultat un système conforme aux objets techniques attendus..   Le script d’import fourni est fonctionnel : il permet l’insertion des données dans le système mis en place.   La documentation technique du script d’import est versionné à la racine du même dépôt Git que celui utilisé pour le script d’import.   Les documentations techniques des script couvrent les parties suivantes : - les dépendances nécessaires pour la réutilisation des scripts (langages, dépendances externes, etc) - les commandes pour l’exécution des scripts.   Le registre des traitements de données personnelles intègre l’ensemble des traitements de données personnelles impliqués dans la base de données.   Les procédures de tri des données personnelles pour la mise en conformité de la base de données avec le RGPD sont rédigées.   Les procédures de tri détaillent les traitements de conformité (automatisés ou non) à appliquer ainsi que leur fréquence d’exécution."


### 'C12 Partager le jeu de donnees en configurant des interfaces logicielles et en creant des interfaces programmables afin de mettre à dispostion le jeu de donnees pour le developpement du projet'

* Critères d'évaluation : "La documentation technique de l'API (REST) couvre tous les points de terminaisons (end points*)  La documentation technique coure les règles d'authentification et/ou d'autorisation de l'API  La documentation technique respecte les standards du modèle choisi (par exemple Open API*)  L’API REST est fonctionnelle pour l’accès aux données du projet : elle restreint par une autorisation (ou authentification) l'accès aux données,  L’API REST est fonctionnelle pour la mise à disposition : elle permet la récupération de l’ensemble des données nécessaires au projet."




# ██ B3 : Data Warehouse


## ═══ E5 : Mettre en place un data warehouse


'C13. Modéliser la structure des données d’un entrepôt de données en s’appuyant sur les dimensions* et les faits* afin d’optimiser l’organisation des données pour les requêtes analytiques.'

* Critères d'évaluation : "Les données nécessaires aux analyses sont listées.  La liste des données nécessaires aux analyses est exhaustive.  Les modélisations logiques et les modélisations physiques sont explicités sans erreur d’interprétation.  Les modélisations appliquent les pratiques de modélisation d’entrepôt de données : en flocon, en étoile, en constellation.  L’approche - top-down* ou bottom-up* de création et modélisation de l’entrepôt de données est justifiée en fonction des caractéristiques du projet, par exemple : volume de données, fréquences des mise à jour nature(s) des analyses..."


### 'C14. Créer un entrepôt de données à partir des paramètres du projet, des contraintes techniques et matérielles et de la modélisation de la structure des données afin de soutenir l’analyse de l’activité et l’aide à la décision stratégique de l’organisation'

* Critères d'évaluation : "L’entrepôt de données remplit les fonctionnalités attendues.  Les configurations principales appliquées sont explicitées.  Les accès aux données opérationnelles sources sont correctement configurés.  Les accès à l’entrepôt de données et/ou datamarts pour les équipes analytiques sont correctement configurés.  La procédure de test est présentée.  La procédure de test couvre l’ensemble du spectre technique et fonctionnel de l’entrepôt de données.  La documentation technique détaille l’architecture technique et couvre la procédure d’installation et de configuration de l’entrepôt de données.  La documentation respecte une structure permettant d’y rechercher rapidement une information spécifique.  La documentation respecte les règles d’accessibilités. Un retour d’expérience est fait concernant la pile technique utilisée au regard des besoins d’analyse et du volume de données géré."


### 'C15. Intégrer les ETL* nécessaires en entrée et en sortie d’un entrepôt de données afin de garantir la qualité et le bon formatage des données en accord avec les modélisations logiques et physiques préalablement établies'

* Critères d'évaluation : "Les formats et le volume des données sont connus et expliqués.  Les ETL sont alimentés avec les données identifiées.  Les formats des zones de sortie sont connus et expliqués.  Les données en sortie respectent le format attendu.  Les ETL appliquent les traitements nécessaires pour la mise en conformité avec les schémas physiques de données des zones de sortie.  Les ETL appliquent les traitements de nettoyage des données utiles et nécessaires à la qualité des jeux de données en sortie : unicité des formats et des unités, détection des doublons, etc...,  Le fonctionnement général et les règles de traitement de chacun des ETL sont clairement explicités, sans ambiguïté."



# ██ B3 : Data warehouse


## ═══ E6 : Maintenir un data warehouse


### 'C16. Gérer l’entrepôt de données à l’aide des outils d’administration et de supervision dans le respect du RGPD, afin de garantir les bons accès, l’intégration des évolutions structurelles et son maintien en condition opérationnelle dans le temps.'

* Critères d'évaluation : "Une journalisation de l’activité de l’entrepôt de données est mise en place.  La journalisation catégorise à minima les alertes et les erreurs.  Un système d’alerte (e-mail, sms, notification..) est mis en place et activé en cas d’erreur notifiée dans les journaux.  Les tâches de maintenance sont priorisées selon les objectifs et les exigences de maintenance.  Les tâches de maintenance sont assignées entre les membres de l’équipe de maintenance.  Les indicateurs de service se base sur les SLA.  Le tableau de bord permet de rendre compte de l’ensemble des indicateurs de service.  Des tâches planifiées de backup partiel et de backup complet du datawarehouse sont programmées et configurées.  Les tâches planifiées produisent les résultats attendus.  La documentation couvre les principaux cas d’usage de gestion de l’entrepôt : l’intégration de nouvelles sources de données, la création de nouveaux accès à l’entrepôt de données, espace de stockage, datamarts, capacité de calcul...  La documentation est structurée par cas d’usage et explicite la mise en œuvre des procédures concernées.  Les nouvelles sources de données sont correctement configurées et ajoutées au processus d’alimentation de l’entrepôt de données.  Les ETL sont mis correctement à jour en fonction.  Les nouveaux accès à l’entrepôt de données sont configurés conformément au besoin.  Le registre des traitements de données personnelles intègre l’ensemble des traitements de données personnelles impliqués dans le projet d’entrepôt de données.  Les procédures de tri des données personnelles pour la mise en conformité de l’entrepôt de données avec le RGPD sont rédigées. Les procédures de tri détaillent les traitements de conformité (automatisés ou non) à appliquer ainsi que leur fréquence d’exécution."


### 'C17. Implémenter des variations dans les dimensions de l’entrepôt de données en appliquant la méthode adaptée en fonction du type de changement demandé afin d’historiser les évolutions de l’activité de l’organisation et maintenir ainsi une bonne capacité d’analyse.'

* Critères d'évaluation : "La modélisation des variations intègre pleinement les changements dans les données sources.  La modélisation des variations permet d’historiser les changements dans les données sources.  Les variations sont intégrées à l’entrepôt de données.  L’intégration des variations respecte la modélisation initiale.  Les ETL sont mis à jour en fonction des besoins liés aux variations.  La documentation est à jour, avec les variations."

## ═══ E7 : Data lake


### 'C18. Concevoir l'architecture du data lake en sélectionnant les technologies appropriées en fonction de la volumétrie, de la variété et de la vitesse des données dans le but de définir l’architecture technique optimale à intégrer.'

* Critères d'évaluation : "Les propositions techniques sont cohérentes avec le cadre d’exploitation.  Le schéma d’architecture tient compte des contraintes de volume, de vitesse et/ou de variété.  Le schéma d’architecture est lisible et utilise un formalisme approprié.  Plusieurs catalogues sont proposés et comparés au regard des contraintes.  L’outil de catalogue sélectionné répond aux contraintes d'exploitabilité et de droits d’accès."


### 'C19. Intégrer les différents composants d'infrastructure du data lake en appliquant la procédure adaptée, afin d’assurer l’acquisition, le stockage et la mise à disposition du catalogue de données'

* Critères d'évaluation : "L’ensemble des éléments de documentation nécessaires à la mise en œuvre de la procédure d’installation sont présentés.  La procédure d’installation se déroule sans erreur dans un environnement de test.  Le système de stockage est installé et fonctionnel en environnement de test.  Les outils batch* et temps réel sont fonctionnels et connectés au système de stockage.  Le catalogue est connecté au système de stockage.  La documentation couvre la procédure d’installation et de configuration du système de stockage, des outils batch et de l’outil de catalogue."


### 'C20. Gérer le catalogue des données en tenant compte de leur nature, de leurs sources d’alimentation et de leur cycle de vie, dans le respect du RGPD, afin de garantir les fonctionnalités du service.'

* Critères d'évaluation : "Les choix des méthodes d’alimentation sont justifiés et appropriés à chaque source de données.  Les scripts d’alimentation s’exécutent sans erreur.  Les données sont importées correctement dans le système de stockage.  Les métadonnées sont intégrées dans le catalogue.  Les procédures de suppression sont conformes aux contraintes d’accès (notamment réglementaires) et aux contraintes opérationnelles.  Le monitorage permet le suivi des conditions matérielles et applicatives.  Le monitorage génère une alerte lors d’une rupture de service.  Le registre des traitements de données personnelles intègre l’ensemble des traitements de données personnelles impliqués dans le projet d’entrepôt de données.  Les procédures de tri des données personnelles pour la mise en conformité de l’entrepôt de données avec le RGPD sont rédigées.  Les procédures de tri détaillent les traitements de conformité (automatisés ou non) à appliquer ainsi que leur fréquence d’exécution."


### 'C21. Implémenter les règles de gouvernance des données en sécurisant la recherche, la récupération et l’ajout de données afin de respecter les règles de gouvernance des données de l’organisation, dans le cadre du déploiement de la politique de protection des données.'

* Critères d'évaluation : "Les droits sont appliqués à des groupes et non à des individus dès que possible.  Les accès fournis répondent aux besoins des groupes concernés.  Les accès fournis sont limités aux ressources nécessaires aux usages des groupes concernés.  Les accès fournis sont conformes à la réglementation (RGPD).  La documentation couvre les groupes d’accès et les droits associés ainsi que les procédures de mise à jour des règles."
